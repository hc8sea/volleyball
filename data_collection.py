# -*- coding: utf-8 -*-
"""data_collection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AFQdefjxbJZcXX361dTf6AfnaEtqMB7E
"""

!pip3 install unidecode
!pip3 install pymysql
!pip install selenium
!apt-get update 
!apt install chromium-chromedriver

import os
import time
import requests

import numpy as np
import pandas as pd

from pprint import pprint
from bs4 import BeautifulSoup
from selenium import webdriver
from unidecode import unidecode

from geopy.geocoders import Nominatim
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as ec

def date_pt(url):
  html = requests.get(url).text
  soup = BeautifulSoup(html ,'html.parser')
  data = soup.find_all(id="Content_Main_LB_DateTime")[0].text

  ano = data.split()[5]
  mes = data.split()[3]
  dia = data.split()[1]

  if mes == 'janeiro':   mes = '01'
  if mes == 'fevereiro': mes = '02'
  if mes == 'março':     mes = '03'
  if mes == 'abril':     mes = '04'
  if mes == 'maio':      mes = '05'
  if mes == 'junho':     mes = '06'
  if mes == 'julho':     mes = '07'
  if mes == 'agosto':    mes = '08'
  if mes == 'setembro':  mes = '09'
  if mes == 'outubro':   mes = '10'
  if mes == 'novembro':  mes = '11'
  if mes == 'dezembro':  mes = '12'
  
  return ano + mes + dia

def get_player(player,list):

  df_pg = pd.read_html(list[0])

  df_1 = df_pg[1]
  df = df_1[df_1['Unnamed: 1_level_0', 'Unnamed: 1_level_1'] == player]

  for url in range (1,3): #forma geral: range(1,len(list)):
    dfs = pd.read_html(list[url])
    
    df1 = dfs[1]
    df2 = dfs[2]
    dfp1 = df1[df1['Unnamed: 1_level_0', 'Unnamed: 1_level_1'] == player]
    dfp2 = df2[df2['Unnamed: 1_level_0', 'Unnamed: 1_level_1'] == player]
    df = df.append(dfp1)
    df = df.append(dfp2)

  return(df)

"""## Criando a tabela allchamp.csv"""

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)

def get_page_sources(clicks):
  list=[]
  driver.implicitly_wait(200)
  start = driver.get("https://superliga.cbv.com.br/tabela-de-jogos-feminino")

  for j in range(clicks):

          driver.implicitly_wait(200)
          driver.find_element(by=By.XPATH, value="//*[@id='matchCalendar']/div[1]/div/div[1]/button").click()
          driver.implicitly_wait(200)


  date = driver.find_elements(by=By.CLASS_NAME, value="day_cell.marked")
  for i in range(len(date)):

          driver.implicitly_wait(200)
          WebDriverWait(driver, 200).until(ec.element_to_be_clickable((By.CLASS_NAME, "day_cell.marked")))
          try:
            driver.find_elements(by=By.CLASS_NAME, value="day_cell.marked")[i].click()
          except:
            continue
          driver.implicitly_wait(200)

          url = driver.page_source
          list.append(url)
  return(list)

list0 = get_page_sources(0)

list1 = get_page_sources(1)

list2 = get_page_sources(2)

list3 = get_page_sources(3)

list4 = get_page_sources(4)

list5 = get_page_sources(5)

lists=[list0,list1,list2,list3,list4]
all_hrefs=[]
for i in lists:                                 #separa as 5 listas
  for j in range(len(i)):                       #separa cada dia do mes
    soup = BeautifulSoup(i[j] ,'html.parser')
    stats = soup.find_all(class_="statistics")
    for k in stats:                             #separa cada jogo individual
      m = k.find_all('a', href=True)[0]['href']
      all_hrefs.append(m)

all_hrefs

#Rode esta célula caso não queira esperar o Scraping terminar. Ela contém o resultado obtido em 18/03/2022

backup_hrefs = ['http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1485',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1483',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1486',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1488',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1487',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1484',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1490',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1491',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1489',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1492',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1493',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1494',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1454',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1455',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1458',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1456',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1457',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1462',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1459',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1461',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1464',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1460',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1463',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1469',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1465',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1466',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1470',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1467',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1468',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1437',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1438',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1436',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1471',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1472',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1474',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1473',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1475',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1476',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1439',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1435',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1440',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1442',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1443',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1445',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1446',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1444',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1441',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1365',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1371',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1355',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1347',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1351',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1347',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1351',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1357',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1354',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1358',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1359',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1362',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1361',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1363',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1360',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1366',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1368',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1367',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1369',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1349',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1356',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1312',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1311',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1313',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1314',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1316',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1315',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1319',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1320',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1322',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1318',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1321',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1317',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1326',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1323',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1325',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1327',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1328',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1324',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1329',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1330',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1331',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1332',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1334',
 'http://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1333']

all_hrefs = backup_hrefs

all_hrefs = list(set(all_hrefs))

df = pd.DataFrame(columns = ['id','homesets','homepts','homeace','homeblock','guestsets','guestpts','guestace','guestblock','home','guest','vincitore'])

for url in all_hrefs:
  try:
    id = url[-4:]
    html = requests.get(url).text
    soup = BeautifulSoup(html ,'html.parser')

    home = soup.find(id="Content_Main_LBL_HomeTeam").text
    homesets = float(soup.find(id="Content_Main_LBL_WonSetHome").text)
    guest = soup.find(id="Content_Main_LBL_GuestTeam").text
    guestsets = float(soup.find(id="Content_Main_LBL_WonSetGuest").text)

    sets = homesets + guestsets

    homesets = homesets
    guestsets = guestsets

    if int(homesets) > int(guestsets):
      vincitore = 1  #meaning home wins
    else:
      vincitore = 0  #meaning guest wins

    tables = pd.read_html(url)
    
    
    for i in range(3):
      if tables[i].shape[1] == 28:
        homepts =     float(tables[i][('Pontos','Tot')].values[-1].replace('-','0'))
        homeace = float(tables[i][('ServiÃ§o','Ace')].values[-1].replace('-','0'))
        homeblock = float(tables[i][ ('Bloqueio','Pts')].values[-1].replace('-','0'))

        guestpts =     float(tables[i+1][('Pontos','Tot')].values[-1].replace('-','0'))
        guestace = float(tables[i+1][('ServiÃ§o','Ace')].values[-1].replace('-','0'))
        guestblock = float(tables[i+1][ ('Bloqueio','Pts')].values[-1].replace('-','0'))
        break

    row = pd.Series([id, homesets/sets, homepts/sets, homeace/sets, homeblock/sets, guestsets/sets, guestpts/sets, guestace/sets, guestblock/sets, unidecode(home), unidecode(guest), vincitore], index=df.columns)

    df = df.append(row,ignore_index=True)
  except:
    print(f'failed here: {url}')
    continue
df

os.makedirs('folder/subfolder', exist_ok=True)  
df.to_csv('folder/subfolder/allchamp.csv')

"""## Criando a tabela map.csv"""

def get_location_by_address(address):
    app = Nominatim(user_agent="tutorial")
    """This function returns a location as raw from an address
    will repeat until success"""
    time.sleep(1)
    try:
        return app.geocode(address).raw
    except:
        return None #{'lat': '25', 'lon': '-71', 'display_name': 'Triângulo das Bermudas'}

lugares=[]
home=[]
homeset=[]
guest=[]
guestset=[]
id = []
for url in list:
  id_ = url[-10:-6]
  
  html = requests.get(url).text
  soup = BeautifulSoup(html ,'html.parser')

  lugares_ = soup.find_all(id="Content_Main_LB_Stadium")[0].text
  home_ = soup.find_all(id="Content_Main_LBL_HomeTeam_sm")[0].text
  homeset_ = soup.find_all(id="Content_Main_LBL_WonSetHome_sm")[0].text
  guest_ = soup.find_all(id="Content_Main_LBL_GuestTeam_sm")[0].text
  guestset_ = soup.find_all(id="Content_Main_LBL_WonSetGuest_sm")[0].text


  lugares.append(lugares_)
  home.append(home_)
  homeset.append(homeset_)
  guest.append(guest_)
  guestset.append(guestset_)
  id.append(id_)


df12 = pd.DataFrame(columns=('ID','LOCAL','HOME','HOMEPTS','GUEST','GUESTPTS','VICTOR'))
df12['LOCAL'] = lugares
df12['HOME'] = home
df12['HOMEPTS'] = homeset
df12['GUEST'] = guest
df12['GUESTPTS'] = guestset
df12['VICTOR'] = df12.apply(lambda x: x['HOME'] if x['HOMEPTS'] > x['GUESTPTS'] else x['GUEST'], axis=1)
df12['ID'] = id
df12['LAT'] = df12.apply(lambda x: get_location_by_address(x['LOCAL'])['lat'] if get_location_by_address(x['LOCAL']) is not None else None, axis=1)
df12['LON'] = df12.apply(lambda x: get_location_by_address(x['LOCAL'])['lon'] if get_location_by_address(x['LOCAL']) is not None else None, axis=1)

list = [

            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1248&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1349&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1316&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1318&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1324&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1333&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1336&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1346&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1364&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1371&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1355&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1446&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1451&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1484&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1487&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1452&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1455&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1461&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1465&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1437&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1474&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1479&ID=14',
            'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1364&ID=14'

            ]

        def get_location_by_address(address):
            app = Nominatim(user_agent="test")
            time.sleep(1)
            try:
                return app.geocode(address).raw
            except:
                  return {'lat': '25', 'lon': '-71', 'display_name': 'Triângulo das Bermudas'}

        lugares=[]
        home=[]
        homeset=[]
        guest=[]
        guestset=[]
        id = []

        for url in list:
          id_ = url[-10:-6]

          html = requests.get(url).text
          soup = BeautifulSoup(html ,'html.parser')

          lugares_ = soup.find_all(id="Content_Main_LB_Stadium")[0].text
          home_ = soup.find_all(id="Content_Main_LBL_HomeTeam_sm")[0].text
          homeset_ = soup.find_all(id="Content_Main_LBL_WonSetHome_sm")[0].text
          guest_ = soup.find_all(id="Content_Main_LBL_GuestTeam_sm")[0].text
          guestset_ = soup.find_all(id="Content_Main_LBL_WonSetGuest_sm")[0].text


          lugares.append(unidecode(lugares_))
          home.append(unidecode(home_))
          homeset.append(homeset_)
          guest.append(unidecode(guest_))
          guestset.append(guestset_)
          id.append(id_)


        df12 = pd.DataFrame(columns=('ID','LOCAL','HOME','HOMEPTS','GUEST','GUESTPTS','VICTOR'))
        df12['LOCAL'] = lugares
        df12['HOME'] = home
        df12['HOMEPTS'] = homeset
        df12['GUEST'] = guest
        df12['GUESTPTS'] = guestset
        df12['VICTOR'] = df12.apply(lambda x: x['HOME'] if x['HOMEPTS'] > x['GUESTPTS'] else x['GUEST'], axis=1)
        df12['ID'] = id
        df12['LAT'] = df12.apply(lambda x: get_location_by_address(x['LOCAL'])['lat'], axis=1)
        df12['LON'] = df12.apply(lambda x: get_location_by_address(x['LOCAL'])['lon'], axis=1)

df12

os.makedirs('folder/subfolder', exist_ok=True)  
df12.to_csv('folder/subfolder/map.csv')

"""## Criando a tabela out.csv"""

list = [
        
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1248&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1349&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1316&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1318&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1324&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1333&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1336&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1346&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1364&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1371&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1355&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1446&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1451&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1484&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1487&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1452&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1455&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1461&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1465&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1437&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1474&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1479&ID=14',
        'https://cbv-web.dataproject.com/MatchStatistics.aspx?mID=1364&ID=14'
            
        ]

dflist=[]
for url in list:
  df = pd.read_html(url)
  id = url[-10:-6]
  html = requests.get(url).text
  soup = BeautifulSoup(html ,'html.parser')
  date_ = soup.find_all(id="Content_Main_LB_DateTime")[0].text
  datetime = date_pt(url)
  df_ = pd.DataFrame(dtype='int')
  for i in range(3):
    if 'Unnamed: 1_level_0' in df[i].columns and 'CAROL GATTAZ' in df[i]['Unnamed: 1_level_0'].values:
        df_ = df[i]
        df_['date'] = pd.Series([date_ for x in range(len(df_.index))])
        df_['datetime'] = pd.Series([datetime for x in range(len(df_.index))])
        df_['ID'] = pd.Series([id for x in range(len(df_.index))])
        dflist.append(df_)
        break

df = dflist[0]
for i in range(1,len(dflist)):
  df = pd.concat([df,dflist[i]])
df

df.columns = df.columns.droplevel(0)
df.head()

df.columns = [
'CAMISA',
'NOME',
'SET_1',
'SET_2',
'SET_3',
'SET_4',
'SET_5',
'PONTOS_TOT',
'PONTOS_BP ',
'PONTOS_VP',
'PTS_TOT',
'SERVICO_TOT',
'SERVICO_ERR',
'SERVICO_ACE',
'RECEPCAO_TOT',
'RECEPCAO_ERR',
'RECEPCAO_PORC_POS',
'RECEPCAO_PORC_EXC',
'RC_TOT',
'RC_PORC_POS',
'ATAQUE_TOT',
'ATAQUE_ERR',
'ATAQUE_BLK',
'ATAQUE_EXC',
'ATAQUE_PORC_EXC',
'AT_EXC',
'BLOQUEIO_PTS',
'BL_PTS',
'DATE',
'DATETIME',
'ID'

]

os.makedirs('folder/subfolder', exist_ok=True)  
df.to_csv('folder/subfolder/out.csv')